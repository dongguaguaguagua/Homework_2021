---
title: "STAT115 Homework 1"
author: "huzongyao"
date: "October 24, 2024"
header-includes:
  - \usepackage{ctex}
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    toc: TRUE
    toc_float:
      collapsed: TRUE
      smooth_scroll: TRUE
    number_sections: TRUE
---

# Part 0: Odyssey Signup

Please fill out the Odyssey survey on Canvas so we can create an account for you

# Part I: Introduction to R

## Problem 1: Installation

**Please install the following R/Bioconductor packages**

```{r install, eval = FALSE}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install()
BiocManager::install("sva")

install.packages(c("ggplot2", "dplyr", "tidyr", "HistData", "mvtnorm",
                   "reticulate"))
```


Please run this command (use `eval=TRUE`) to see if Bioconductor can work fine.

```{r, eval=FALSE}
BiocManager::valid()
```


```{r libraries, message = FALSE}
# these packages are needed for HW2
# affy and affyPLM are needed to read the microarray data and run RMA
library(sva) # for batch effect correction. Contains ComBat and sva.
library(ggplot2) # for plotting
library(dplyr) # for data manipulation
library(reticulate) # needed to run python in Rstudio
# these next two are not essential to this course
library(mvtnorm) # need this to simulate data from multivariate normal
library(HistData) # need this for data
```


## Problem 2: Getting help

You can use the `mean()` function to compute the mean of a vector like
so:

```{r mean}
x1 <- c(1:10, 50)
mean(x1)
```

However, this does not work if the vector contains NAs:

```{r mean-na}
x1_na <- c(1:10, 50, NA)
mean(x1_na)
```

**Please use R documentation to find the mean after excluding NA's (hint: `?mean`)**

```{r problem2}
remove_na <- function(vec) {
    # Remove NaN values from the vector
    return(vec[!is.na(vec)])
}

# Example usage
x1_na <- c(1:10, 50, NA)
mean(x1_na)
remove_na(x1_na) %>% mean()
```


Grading: Grade on correctness.

+ 0.5pt

# Part II: Data Manipulation

## Problem 3: Basic Selection

In this question, we will practice data manipulation using a dataset
collected by Francis Galton in 1886 on the heights of parents and their
children. This is a very famous dataset, and Galton used it to come up
with regression and correlation.

The data is available as `GaltonFamilies` in the `HistData` package.
Here, we load the data and show the first few rows. To find out more
information about the dataset, use `?GaltonFamilies`.

```{r loadGalton}
data(GaltonFamilies)
head(GaltonFamilies)
```

a. **Please report the height of the 10th child in the dataset.**

```{r problem3a}
# report the height of the 10th child in the dataset.
GaltonFamilies$childHeight[10]
```

b. **What is the breakdown of male and female children in the dataset?**

```{r problem3b}
# the breakdown of male and female children in the dataset
table(GaltonFamilies$gender)
```

c. **How many observations (number of rows) are in Galton's dataset? Please answer this
question without consulting the R help.**

```{r problem3c}
# number of rows in Galton's dataset
nrow(GaltonFamilies)
```

d. **What is the mean height for the 1st child in each family?**

```{r problem3d}
# the mean height for the 1st child in each family
GaltonFamilies %>%
    group_by(family) %>%
    summarise(meanHeight = mean(childHeight[1]))
```

e. **Create a table showing the mean height for male and female children.**

```{r problem3e}
GaltonFamilies %>%
    group_by(gender) %>%
    summarise(meanHeight = mean(childHeight))
```

f. **What was the average number of children each family had?**

```{r problem3f}
# the average number of children each family had
GaltonFamilies %>%
    group_by(family) %>%
    # count the number of children in each family
    summarise(nChildren = n()) %>%
    # create a new column with the mean number of children
    summarise(meanChildren = mean(nChildren))
```

g. **Convert the children's heights from inches to centimeters and store
it in a column called `childHeight_cm` in the `GaltonFamilies` dataset.
Show the first few rows of this dataset.**

```{r problem3g}
GaltonFamilies$childHeight_cm <- GaltonFamilies$childHeight * 2.54
head(GaltonFamilies)
```

## Problem 4: Spurious Correlation

```{r gen-data-spurious, cache = TRUE, eval=TRUE}
# set seed for reproducibility
set.seed(1234)
N <- 25
ngroups <- 100000
sim_data <- data.frame(group = rep(1:ngroups, each = N),
                       X = rnorm(N * ngroups),
                       Y = rnorm(N * ngroups))
```

In the code above, we generate `r ngroups` groups of `r N` observations
each. In each group, we have X and Y, where X and Y are independent
normally distributed data and have 0 correlation.

a. **Find the correlation between X and Y for each group, and display
the highest correlations.**

Hint: since the data is quite large and your code might take a few
moments to run, you can test your code on a subset of the data first
(e.g. you can take the first 100 groups like so):

```{r subset}
# the first 100 groups
sim_data_subset <- sim_data[sim_data$group <= 100, ]
# plot the sim_data_subset
# plot(sim_data_subset$X, sim_data_subset$Y)
# Find the correlation between X and Y for each group, and display the highest correlations.
sim_data_subset %>%
    # divide into 100 groups
    group_by(group) %>%
    # calculate the correlation between X and Y for each group
    summarise(correlation = cor(X, Y)) %>%
    # sort the correlations in descending order
    arrange(desc(correlation)) %>%
    head()
```

In general, this is good practice whenever you have a large dataset:
If you are writing new code and it takes a while to run on the whole
dataset, get it to work on a subset first. By running on a subset, you
can iterate faster.

However, please do run your final code on the whole dataset.

```{r cor, cache = TRUE}
sim_data %>%
    group_by(group) %>%
    summarise(correlation = cor(X, Y)) %>%
    arrange(desc(correlation)) %>%
    head()
```

b. **The highest correlation is around 0.8. Can you explain why we see
such a high correlation when X and Y are supposed to be independent and
thus uncorrelated?**

> cherrypicked: To choose only the best

Because we cherrypicked the highest correlations among 100,000
correlations, it is just by chance that we found a few with such
a high correlation. We can see in the histogram below that most of
the correlations are around the expected value of 0.

```{r cor-hist, eval=F}
sim_data %>%
    group_by(group) %>%
    summarise(correlation = cor(X, Y)) %>%
    # plot the histogram of the correlations
    # aes: aesthetic mapping
    ggplot(aes(x = correlation)) + geom_histogram(bins = 30)
```


# Part III: Plotting

## Problem 5

**Show a plot of the data for the group that had the highest correlation
you found in Problem 4.**

```{r problem5}
# the data that had the highest correlation
sim_data %>%
    filter(group == 99655) %>%
    ggplot(aes(x = X, y = Y)) +
    geom_point() +
    ggtitle("Scatter plot of X and Y for group 99655")
```

Grading: 1pt.

## Problem 6

We generate some sample data below. The data is numeric, and has 3
columns: X, Y, Z.

```{r gen-data-corr}
N <- 100
# 定义协方差矩阵 (二维正态分布的相关性和方差)
Sigma <- matrix(c(1, 0.75, 0.75, 1), nrow = 2, ncol = 2) * 1.5
# 五组数据的均值坐标，这些均值构成了五个不同的正态分布中心
means <- list(c(11, 3), c(9, 5), c(7, 7), c(5, 9), c(3, 11))
# lapply: 对列表 means 中的每个元素执行操作，即对每个正态分布生成 N 个数据点
# rmvnorm: 生成多元正态分布数据
dat <- lapply(means, function(mu) rmvnorm(N, mu, Sigma))
# 将合并后的矩阵转换为数据框 (data.frame)
dat <- as.data.frame(Reduce(rbind, dat)) %>%
    # 为数据框新增一列 Z，用来标记每组数据点属于哪一个分布
    # seq_along(means): [1, 2, 3, 4, 5]
    # rep(sequence, each = N): 将 sequence 中的每个数重复 N 次，并将所有结果连接在一起
    # mutate(): dplyr 包中的一个函数，用于在数据框中添加或修改列
    mutate(Z = as.character(rep(seq_along(means), each = N)))
# 将数据框的列命名为 X、Y 和 Z (Z 是数据点的类别 1~5)
names(dat) <- c("X", "Y", "Z")
```

a. **Compute the overall correlation between X and Y.**

```{r problem6a}
dat %>%
    summarise(correlation = cor(X, Y))
```

b. **Make a plot showing the relationship between X and Y. Comment on
the correlation that you see.**

```{r problem6b}
# Make a plot showing the relationship between X and Y.
dat %>%
    ggplot(aes(x = X, y = Y)) +
    geom_point() +
    ggtitle("Scatter plot of X and Y")
```

> 根据图中的点的分布，可以很明显的看出 X 和 Y 是负相关的，但是这些点显然地分布在5组中。

The correlation between X and Y is negative.

c. **Compute the correlations between X and Y for each level of Z.**

```{r problem6c}
# Compute the correlations between X and Y for each level of Z.
dat %>%
    group_by(Z) %>%
    summarise(correlation = cor(X, Y))
```

d. **Make a plot showing the relationship between X and Y, but this
time, color the points using the value of Z. Comment on the result,
especially any differences between this plot and the previous plot.**

```{r problem6d}
dat %>%
    ggplot(aes(x = X, y = Y, color = Z)) +
    geom_point() +
    ggtitle("Scatter plot of X and Y colored by Z")
```

> 这是一个经典的辛普森悖论的例子，虽然一眼看上去 X 和 Y 呈负相关，但实际上当我们考虑 Z 变量时，在 Z 相同时，X 和 Y 是正相关的。

# Part IV: Bash practices

## Problem 7: Bash practices on Odyessy

Please answer the following question using bash commands and include those in
your answer. Data are available at `/n/stat115/2020/HW1/public_MC3.maf`

Mutation Annotation Format ([MAF](https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/))
is a tab-delimited text file with aggregated mutation information.
MC3.maf `/n/stat115/2020/HW1/public_MC3.maf` is a curated list of [somatic mutation](https://www.britannica.com/science/somatic-mutation)
occured in many patients with different types of cancers from TCGA (癌症基因组图谱).

> 由于GitHub上没有找到public_MC3.maf，我在[mc3-2017](https://gdc.cancer.gov/about-data/publications/mc3-2017)中找到了相同的数据：[下载链接](https://api.gdc.cancer.gov/data/1c8cfe5f-e52d-41ba-94da-f15ea1337efc)
> mc3.v0.2.8.PUBLIC.maf 保存在 ${project}/assets/mc3.v0.2.8.PUBLIC.maf
> 由于源文件过大，无法使用正常的编辑器打开，我不得不使用 UltraEdit

Since a complete MAF file contains far more information than we need,
in this problem we will focus on part of it.

```
Chromosome	Start_Position	Hugo_Symbol	Variant_Classification
10	123810032	TACC2	Missense_Mutation
10	133967449	JAKMIP3	Silent
11	124489539	PANX3	Missense_Mutation
11	47380512	SPI1	Missense_Mutation
11	89868837	NAALAD2	Missense_Mutation
11	92570936	FAT3	Silent
12	107371855	MTERFD3	Missense_Mutation
12	108012011	BTBD11	Missense_Mutation
12	117768962	NOS1	5'Flank
```

In  `/n/stats115/2020/HW1/MC3/public_MC3.maf`, `Chromosome` and `Start_Position`
together specifies the genomics location where a location has happened.
`Hogo_symbol` is the overlapping gene of that location, and
`Variant_Classification` specifies how it influences downstream biological
processes, e.g. transcription and translation.

Please include your bash commands and the full output from bash console
with text answer to the questions.


a. How many lines are there in this file? How many times "KRAS" gene has emerged?

```{r q7a, engine="bash", eval = FALSE}
wc -l ../assets/mc3.v0.2.8.PUBLIC.maf # lines
grep -c KRAS ../assets/mc3.v0.2.8.PUBLIC.maf # KRAS
```

```
3600964 ../assets/mc3.v0.2.8.PUBLIC.maf
954
```

b. How many unique `Variant_Classification` are there in the MAF? Please
count occurence of each type and sort them. Which one is the most frequent?

```{r q7b, engine="bash", eval = FALSE}
# head -n 1 ../assets/mc3.v0.2.8.PUBLIC.maf
# 由于我下载的数据比较全的原因，第9列是 Variant_Classification
cut -f 9 ../assets/mc3.v0.2.8.PUBLIC.maf | sort | uniq -c | sort -nr
```

```
1921979 Missense_Mutation
782687 Silent
282636 3'UTR
157232 Nonsense_Mutation
108104 Intron
87013 Frame_Shift_Del
81323 5'UTR
50617 Splice_Site
49540 RNA
27128 Frame_Shift_Ins
21060 3'Flank
15726 5'Flank
10254 In_Frame_Del
2723 Translation_Start_Site
2042 Nonstop_Mutation
 899 In_Frame_Ins
```

> Missense_Mutation is the most frequent type of mutation.

c. What are the top FIVE most frequent genes? Please provide
the bash command and equivalent Python command. If you are a PI
looking for a gene to investigate (you need to find a gene with potentially
better biological significance), which gene out of the top 5 would you
choose? Why?

```{r q7c, engine="bash", eval = FALSE}
# head -n 1 ../assets/mc3.v0.2.8.PUBLIC.maf
# 第1列是 Hugo_Symbol
cut -f 1 ../assets/mc3.v0.2.8.PUBLIC.maf | sort | uniq -c | sort -nr | head -n 5
```

```
15171 TTN
6875 MUC16
4601 TP53
3198 CSMD3
3102 SYNE1
```

Equivalent python command:

```{r q7cpy, engine="python", eval=FALSE}
import numpy as np
import pandas as pd
# read ../assets/mc3.v0.2.8.PUBLIC.maf
mc3_maf = pd.read_csv("../assets/mc3.v0.2.8.PUBLIC.maf", sep="\t")
# Lines in this file
print(mc3_maf.shape[0])
# count occurence of each unique Variant_Classification and sort them.
print(mc3_maf["Variant_Classification"].value_counts().sort_values(ascending=False))
# FIVE most frequent genes
print(mc3_maf["Hugo_Symbol"].value_counts().head(5))
```

```
3600963

Variant_Classification
Missense_Mutation         1921979
Silent                     782687
3'UTR                      282636
Nonsense_Mutation          157232
Intron                     108104
Frame_Shift_Del             87013
5'UTR                       81323
Splice_Site                 50617
RNA                         49540
Frame_Shift_Ins             27128
3'Flank                     21060
5'Flank                     15726
In_Frame_Del                10254
Translation_Start_Site       2723
Nonstop_Mutation             2042
In_Frame_Ins                  899
Name: count, dtype: int64

Hugo_Symbol
TTN      15171
MUC16     6875
TP53      4601
CSMD3     3198
SYNE1     3102
Name: count, dtype: int64
```

> 如果我是一个PI（Principal Investigator），我会选取 TP53 进行研究，因为经过查询我得知 TP53 是一个非常重要的抑癌基因。它被称为“基因组的守护者”，在细胞周期调控、DNA损伤修复和诱导细胞凋亡中起着至关重要的作用，也是很多现代抗癌药物的靶点。

d. Write a bash program that determines whether a user-input year ([YYYY]) is
a leap year or not (all years that are multiples of four. If the year is
centennial and not divisible by 400, then it is not a leap year).
The user input can be either positional or interactive.
Please include the content of your shell script here and test on
1900/2000/2002, does your code run as expected?

```{r q7d, engine="bash", eval = FALSE}
#!/bin/bash

# Function to check if the year is a leap year
is_leap_year() {
    local year=$1
    if (( year % 400 == 0 )); then
        echo "$year is a leap year."
    elif (( year % 100 == 0 )); then
        echo "$year is not a leap year."
    elif (( year % 4 == 0 )); then
        echo "$year is a leap year."
    else
        echo "$year is not a leap year."
    fi
}

# Main script logic
if [ -z "$1" ]; then
    # Interactive mode
    read -p "Enter a year: " year
    is_leap_year "$year"
else
    # Positional mode
    for year in "$@"; do
        is_leap_year "$year"
    done
fi

# Test by:
# ./leap_year.sh 1900 2000 2002
```


# Part V. High throughput sequencing read mapping

We will give you a simple example to test high throughput sequencing
alignment for RNA-seq data. Normally for paired-end sequencing data,
each sample will have two separate FASTQ files, with line-by-line
correspondence to the two reads from the same fragment. Read mapping
could take a long time, so we have created just two FASTQ files of one
RNA-seq sample with only 3M fragments (2 * 3M reads) for you to run STAR
instead of the full data. The files are located at
`/n/stat115/2020/HW1`. The mapping will generate one single output
file. Make sure to use the right parameters for single-end (SE) vs
paired-end (PE) modes in BWA and STAR.

Please include the commands that you used to run BWA and STAR in your
answers.


## Problem 8: BWA

1. Use BWA (Li & Durbin, Bioinformatics 2009) to map the reads to the
Hg38 version of the reference genome, available on Odyssey at
`/n/stat115/HW2_2019/bwa_hg38_index/hg38.fasta`. In
`/n/stat115/HW1_2020/BWA/loop`, you are provided with three `.fastq`
files with following structure (`A_l` and `A_r` are paired sequencing reads
from sample_A). Write a for loop in bash to align reads to the reference
using BWA PE mode and geneterate output in SAM format.

How many rows are in each output `.sam` files? Use SAMTools on the output
to find out how many reads are mappable and uniquely mappable
(please also calculate the ratio). Please include full samtools output
and text answer.


```{r 8, engine="bash", eval = FALSE}
# please provide the content of your sbatch script (including the header)
```

```
samtools output
```

You text answer

## Problem 9: STAR alignment

1. Use STAR (Dobin et al, Bioinformatics 2012) to map the reads to the
reference genome, available on Odyssey at
`/n/stat115/HW1_2020/STARIndex`. Use the paired-end alignment mode and
generate the output in SAM format. Please include full STAR report.
How many reads are mappable and how many are uniquely mappable?

```{r 9, engine="bash", eval = FALSE}
# please provide the content of your sbatch script (including the header)
```

```
Log file from STAR
```
Yor text answer here.


2. If you are getting a different number of mappable fragments between
BWA and STAR on the same data, why?

Your text answer here.


# Part VII: Dynamic programming with Python

## Problem 10

Given a list of finite integer numbers,
Write a python script to maximize the Z where Z is the sum of the
numbers from location X to location Y on this list. Be aware, your
algorithm should look at each number ONLY ONCE from left to right.
Your script should return three values: the starting index location X,
the ending index location Y, and Z, the sum of numbers between index
X and Y (inclusive).

For example, if A=[-2, 1, 7, -4, 5, 2, -3, -6, 4, 3, -8, -1, 6, -7, -9, -5],
your program should return (start_index = 1, end_index = 5, sum = 11)
corresponding to [1, 7, -4, 5, 2].

Please test your program with this example and see if you can get the
correct numbers.

Hint: Consider dynamic programming.

```{python dynamic-programming, eval=TRUE, echo = TRUE}
def max_subarray(l):
    start = end = current_start = current_sum = 0
    max_sum = l[0]
    for i, a in enumerate(l):
        if current_sum <= 0:
            current_sum, current_start = 0, i
        current_sum += a
        if current_sum > max_sum:
            max_sum, start, end = current_sum, current_start, i
    return start, end, max_sum


print(max_subarray([-2, 1, 7, -4, 5, 2, -3, -6, 4, 3, -8, -1, 6, -7, -9, -5]))
```

> 这道题其实用贪心算法做空间复杂度会更小（O(n) -> O(1)）。时间复杂度都是 O(n)。思路是这样：如果最大子序列的前1项、前2项和、一直到前n项和中只要有大于0的，那么它一定就不是最大子序列，因此最大子序列的前n项和必然<=0。这时我们可以用前缀和计算出所有n，使得前n项<=0，即找到所有可能的最大子序列的起始位置。再用贪心算法不断累加后面的数，遍历的过程中更新最大子序列的和。
